{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845626f3-356b-4757-b4b8-1a2077682192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.2.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\paul\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\paul\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\paul\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.1/11.5 MB 11.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.5/11.5 MB 11.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.8/11.5 MB 12.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.2/11.5 MB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.3-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.9/12.6 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.8/12.6 MB 14.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.9/12.6 MB 14.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.8/12.6 MB 14.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 13.9 MB/s eta 0:00:00\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.3 pandas-2.2.3 tzdata-2025.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6895a413-152b-46f6-8d8c-5bcbc8fc9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736d3c58-6a93-42f9-8064-2d28be08671f",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'reddit/submissions/RS_2025-01/RS_2025-01'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m json_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexample.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreddit/submissions/RS_2025-01/RS_2025-01\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(f\u001b[38;5;241m.\u001b[39mreadlines))\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'reddit/submissions/RS_2025-01/RS_2025-01'"
     ]
    }
   ],
   "source": [
    "json_filename = 'example.json'\n",
    "\n",
    "count = 0\n",
    "with open('reddit/submissions/RS_2025-01/RS_2025-01', 'r', encoding='utf-8') as f:\n",
    "    print(len(f.readlines))\n",
    "    for line in f:\n",
    "        line = json.loads(line)\n",
    "        if len(line['all_awardings']) == 0:\n",
    "            count += 1\n",
    "            if count % 10000 == 0:\n",
    "                print(f'At line {count}')\n",
    "                print(line['all_awardings'])\n",
    "            continue\n",
    "        if not os.path.isfile(json_filename):\n",
    "            data = []\n",
    "            with open(json_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "            \n",
    "        with open(json_filename, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(line, json_file, indent=4)\n",
    "        print(f'Number of attributes: {len(line.keys())}')\n",
    "        break\n",
    "\n",
    "print(f'Wrote example line to {json_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0e50ad-9205-4a32-8149-ef7ac55851db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_lines(input_file, output_file, lines):\n",
    "    file_content = []\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line_count, line in enumerate(tqdm(f, total=lines, desc=\"Reading lines\"), start=1):\n",
    "            if line_count > lines:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                cleaned_line = json.loads(line)  # Parse JSON\n",
    "                file_content.append(cleaned_line)\n",
    "            except:\n",
    "                continue  # Skip invalid JSON lines\n",
    "            \n",
    "    print(f'Writing {output_file}...')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(file_content, f, indent=4, ensure_ascii=False)  # Remove extra backslashes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a3df8b-d892-4b1a-b04b-7ea8aaa4777f",
   "metadata": {},
   "source": [
    "# Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ce58cd-fb08-41d7-ae33-e1e78b50be5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading lines: 100%|███████████████| 1000000/1000000 [00:23<00:00, 42263.15it/s]\n"
     ]
    }
   ],
   "source": [
    "comment_file = 'reddit/comments/RC_2025-01/RC_2025-01'\n",
    "write_lines(comment_file, 'lines.json', 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442f9697-5a04-410f-9213-7fa35bcd2a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_authors(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    authors = set()\n",
    "    for i in data:\n",
    "        author = i['author']\n",
    "        if author != '[deleted]':\n",
    "            authors.add(author)\n",
    "\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cdb9aea-a866-4264-b645-8d1efcfa407a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460645\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_authors('lines.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0422a312-e81b-47fb-b4c0-9a4011f304e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deleted_count(json_file):\n",
    "    with open('lines.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    del_count = 0\n",
    "    for i in tqdm(data, desc='Reading data...'):\n",
    "        if i['author'] == '[deleted]':\n",
    "            del_count += 1\n",
    "    return del_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c5eb1c7-70ec-4d1d-90e2-b2d86a763eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading data...: 100%|███████████| 1000000/1000000 [00:00<00:00, 1629128.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32769"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_deleted_count('lines.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6249314-60d6-4047-b250-4c5c82b93ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lines.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f03fa05-17c2-40f3-acb9-1fe9d68a0079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_meta: {'retrieved_2nd_on': 1735819210}\n",
      "all_awardings: []\n",
      "approved_at_utc: None\n",
      "approved_by: None\n",
      "archived: False\n",
      "associated_award: None\n",
      "author: Alone_Friendship4618\n",
      "author_flair_background_color: None\n",
      "author_flair_css_class: None\n",
      "author_flair_richtext: []\n",
      "author_flair_template_id: None\n",
      "author_flair_text: None\n",
      "author_flair_text_color: None\n",
      "author_flair_type: text\n",
      "author_fullname: t2_94fk7riv\n",
      "author_is_blocked: False\n",
      "author_patreon_flair: False\n",
      "author_premium: False\n",
      "awarders: []\n",
      "banned_at_utc: None\n",
      "banned_by: None\n",
      "body: Creation change and miracle\n",
      "can_gild: False\n",
      "can_mod_post: False\n",
      "collapsed: False\n",
      "collapsed_because_crowd_control: None\n",
      "collapsed_reason: None\n",
      "collapsed_reason_code: None\n",
      "comment_type: None\n",
      "controversiality: 0\n",
      "created: 1735689600\n",
      "created_utc: 1735689600\n",
      "distinguished: None\n",
      "downs: 0\n",
      "edited: False\n",
      "gilded: 0\n",
      "gildings: {}\n",
      "id: m4rvtu0\n",
      "is_submitter: False\n",
      "likes: None\n",
      "link_id: t3_1hqah4t\n",
      "locked: False\n",
      "mod_note: None\n",
      "mod_reason_by: None\n",
      "mod_reason_title: None\n",
      "mod_reports: []\n",
      "name: t1_m4rvtu0\n",
      "no_follow: True\n",
      "num_reports: None\n",
      "parent_id: t3_1hqah4t\n",
      "permalink: /r/inspirationalquotes/comments/1hqah4t/what_three_words_do_you_see_below_credits_tim_han/m4rvtu0/\n",
      "removal_reason: None\n",
      "replies: \n",
      "report_reasons: None\n",
      "retrieved_on: 1735689617\n",
      "saved: False\n",
      "score: 1\n",
      "score_hidden: False\n",
      "send_replies: True\n",
      "stickied: False\n",
      "subreddit: inspirationalquotes\n",
      "subreddit_id: t5_2tu2k\n",
      "subreddit_name_prefixed: r/inspirationalquotes\n",
      "subreddit_type: public\n",
      "top_awarded_type: None\n",
      "total_awards_received: 0\n",
      "treatment_tags: []\n",
      "unrepliable_reason: None\n",
      "ups: 1\n",
      "user_reports: []\n"
     ]
    }
   ],
   "source": [
    "keys = []\n",
    "for i in data[0].keys():\n",
    "    print(f'{i}: {data[0][i]}')\n",
    "    keys.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47ed5394-5662-4a3c-8e1e-f57971adeba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.4\n"
     ]
    }
   ],
   "source": [
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a800fee1-c8f9-4e71-9379-14b4e4002261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting authors: 100%|█████████| 1000000/1000000 [00:03<00:00, 282052.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB saved to reddit_db.db successfully!\n"
     ]
    }
   ],
   "source": [
    "sqlite_path = 'reddit_db.db'\n",
    "\n",
    "unique_authors = set()  # Set to keep track of authors\n",
    "filtered_data = []\n",
    "\n",
    "for item in tqdm(data, desc='Converting authors'):\n",
    "    # Check if the 'author' key exists in the item\n",
    "    if 'author_fullname' in item:\n",
    "        author = item['author_fullname']\n",
    "        if author not in unique_authors and item['author'] != '[deleted]':\n",
    "            unique_authors.add(author)\n",
    "            filtered_item = {}\n",
    "            for k, v in item.items():\n",
    "                if not k.startswith('author'):\n",
    "                    continue\n",
    "                if type(v) == dict:\n",
    "                    v = str(v)\n",
    "                if type(v) == list:\n",
    "                    if len(v) == 0:\n",
    "                        v = None\n",
    "                    else:\n",
    "                        v = str(v)\n",
    "                filtered_item[k] = v \n",
    "            # filtered_item = {k: v for k, v in item.items() if k.startswith('author')}\n",
    "            filtered_data.append(filtered_item)\n",
    "\n",
    "conn = sqlite3.connect(sqlite_path)\n",
    "df = pd.DataFrame([{k: list(v) if isinstance(v, np.ndarray) else v for k, v in row.items()} for row in filtered_data])\n",
    "df.to_sql('authors', conn, if_exists='replace', index=False)\n",
    "conn.close()\n",
    "print(f'DB saved to {sqlite_path} successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab91dfb-499f-45ec-9803-b7a430b0aec1",
   "metadata": {},
   "source": [
    "# Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d3f605-805a-44d2-8189-2c2f9ccc0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_batch_lines(input_file, output_folder):\n",
    " \n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Read and write in chunks of 10,000 lines\n",
    "    batch_size = 10000\n",
    "    batch_number = 1\n",
    "    lines_batch = []\n",
    "    \n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for line in tqdm(infile, desc='Writing lines...'):\n",
    "            lines_batch.append(line.strip())\n",
    "    \n",
    "            # When batch reaches 10,000 lines, write to a JSON file\n",
    "            if len(lines_batch) == batch_size:\n",
    "                output_file = os.path.join(output_folder, f\"batch_{batch_number}.json\")\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "                    json.dump(lines_batch, outfile, indent=4)\n",
    "                \n",
    "                # Reset for next batch\n",
    "                lines_batch = []\n",
    "                batch_number += 1\n",
    "    \n",
    "    # Write remaining lines if any\n",
    "    if lines_batch:\n",
    "        output_file = os.path.join(output_folder, f\"batch_{batch_number}.json\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            json.dump(lines_batch, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2570ebf7-420c-4eb0-8154-594a9e44d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inplace_authors(line, json_file, authors_processed, batch_size=1000):\n",
    "    if not os.path.isfile(json_file):\n",
    "        data = []\n",
    "        with open(json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "    \n",
    "    line = json.loads(line)\n",
    "    \n",
    "    if line['author'] != '[deleted]':\n",
    "        if line['author_fullname'] not in authors_processed:\n",
    "            with open(json_file, encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract author-related keys\n",
    "            author_data = {}\n",
    "            for k, v in line.items():\n",
    "                if k.startswith('author'):\n",
    "                    if isinstance(v, list):\n",
    "                        v = None if not v else str(v)\n",
    "                    if isinstance(v, dict):\n",
    "                        v = str(v)\n",
    "                author_data[k] = v\n",
    "            \n",
    "            # Append the new author data\n",
    "            data.append(author_data)\n",
    "            \n",
    "            # Write back to the JSON file only every batch_size iterations\n",
    "            if len(data) % batch_size == 0:\n",
    "                with open(json_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "            # Mark this author as processed\n",
    "            authors_processed.add(line['author_fullname'])\n",
    "    \n",
    "    return authors_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f8fafc0f-fd2c-4073-8bf2-de2c147846c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing authors:   0%|                                                                | 0/331981952 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 38299 column 192 (char 1558048)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(submission_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tqdm(f, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing authors\u001b[39m\u001b[38;5;124m'\u001b[39m, total\u001b[38;5;241m=\u001b[39mtotal):\n\u001b[1;32m----> 8\u001b[0m         authors_processed \u001b[38;5;241m=\u001b[39m inplace_authors(line, json_file, authors_processed)\n",
      "Cell \u001b[1;32mIn[80], line 12\u001b[0m, in \u001b[0;36minplace_authors\u001b[1;34m(line, json_file, authors_processed, batch_size)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor_fullname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m authors_processed:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 12\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Extract author-related keys\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     author_data \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 38299 column 192 (char 1558048)"
     ]
    }
   ],
   "source": [
    "submission_file = 'reddit/submissions/RS_2025-01/RS_2025-01'\n",
    "json_file = 'authors.json'\n",
    "total = 331981952\n",
    "# write_lines(submission_file, 'lines_sub.json', 1000000)\n",
    "authors_processed = set([])\n",
    "with open(submission_file, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f, desc='Processing authors', total=total):\n",
    "        authors_processed = inplace_authors(line, json_file, authors_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a91f30-de66-4a94-aa4c-a3af76d9d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lines_sub.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f57785a5-17ca-447e-a18a-20f007c7f0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total 106 keys\n",
      "_meta:{'retrieved_2nd_on': 1735819210}\n",
      "all_awardings:[]\n",
      "allow_live_comments:False\n",
      "approved_at_utc:None\n",
      "approved_by:None\n",
      "archived:False\n",
      "author:NBA_MOD\n",
      "author_flair_background_color:#edeff1\n",
      "author_flair_css_class:NBA\n",
      "author_flair_richtext:[{'e': 'text', 't': ' r/NBA'}]\n",
      "author_flair_template_id:e5aa3fb6-3feb-11e8-8409-0ef728aaae7a\n",
      "author_flair_text: r/NBA\n",
      "author_flair_text_color:dark\n",
      "author_flair_type:richtext\n",
      "author_fullname:t2_6vjwa\n",
      "author_is_blocked:False\n",
      "author_patreon_flair:False\n",
      "author_premium:False\n",
      "awarders:[]\n",
      "banned_at_utc:None\n",
      "banned_by:None\n",
      "can_gild:False\n",
      "can_mod_post:False\n",
      "category:None\n",
      "clicked:False\n",
      "content_categories:None\n",
      "contest_mode:False\n",
      "created:1735689600\n",
      "created_utc:1735689600\n",
      "discussion_type:None\n",
      "distinguished:None\n",
      "domain:self.nba\n",
      "downs:0\n",
      "edited:False\n",
      "gilded:0\n",
      "gildings:{}\n",
      "hidden:False\n",
      "hide_score:False\n",
      "id:1hqr92b\n",
      "is_created_from_ads_ui:False\n",
      "is_crosspostable:True\n",
      "is_meta:False\n",
      "is_original_content:False\n",
      "is_reddit_media_domain:False\n",
      "is_robot_indexable:True\n",
      "is_self:True\n",
      "is_video:False\n",
      "likes:None\n",
      "link_flair_background_color:#ff4500\n",
      "link_flair_css_class:gamethread\n",
      "link_flair_richtext:[{'e': 'text', 't': 'Game Thread'}]\n",
      "link_flair_template_id:0267aa0a-5c54-11e4-a8b9-12313b0b3108\n",
      "link_flair_text:Game Thread\n",
      "link_flair_text_color:light\n",
      "link_flair_type:richtext\n",
      "locked:False\n",
      "media:None\n",
      "media_embed:{}\n",
      "media_only:False\n",
      "mod_note:None\n",
      "mod_reason_by:None\n",
      "mod_reason_title:None\n",
      "mod_reports:[]\n",
      "name:t3_1hqr92b\n",
      "no_follow:True\n",
      "num_comments:808\n",
      "num_crossposts:0\n",
      "num_reports:None\n",
      "over_18:False\n",
      "permalink:/r/nba/comments/1hqr92b/game_thread_minnesota_timberwolves_1714_oklahoma/\n",
      "pinned:False\n",
      "pwls:6\n",
      "quarantine:False\n",
      "removal_reason:None\n",
      "removed_by:None\n",
      "removed_by_category:None\n",
      "report_reasons:None\n",
      "retrieved_on:1735689617\n",
      "saved:False\n",
      "score:21\n",
      "secure_media:None\n",
      "secure_media_embed:{}\n",
      "selftext:##General Information\n",
      "    **TIME**     |**MEDIA**                            |**Team Subreddits**        |\n",
      "    :------------|:------------------------------------|:-------------------|\n",
      "    08:00 PM Eastern |**Game Preview**: [NBA.com](https://www.nba.com/game/MIN-vs-OKC-0022400452/preview) | /r/timberwolves          |\n",
      "    07:00 PM Central |**Game Charts**: [NBA.com](https://www.nba.com/game/MIN-vs-OKC-0022400452/game-charts) | /r/thunder           |\n",
      "    06:00 PM Mountain|**Play By Play**: [NBA.com](https://www.nba.com/game/MIN-vs-OKC-0022400452/play-by-play)|               |\n",
      "    05:00 PM Pacific |**Box Score**: [NBA.com](https://www.nba.com/game/MIN-vs-OKC-0022400452/boxscore) |                 |\n",
      "   [Reddit Stream](https://reddit-stream.com/comments/auto) (You must click this link from the comment page.)\n",
      "send_replies:False\n",
      "spoiler:False\n",
      "stickied:False\n",
      "subreddit:nba\n",
      "subreddit_id:t5_2qo4s\n",
      "subreddit_name_prefixed:r/nba\n",
      "subreddit_subscribers:14298761\n",
      "subreddit_type:public\n",
      "suggested_sort:new\n",
      "thumbnail:self\n",
      "thumbnail_height:None\n",
      "thumbnail_width:None\n",
      "title:GAME THREAD: Minnesota Timberwolves (17-14) @ Oklahoma City Thunder (26-5) - (January 01, 2025)\n",
      "top_awarded_type:None\n",
      "total_awards_received:0\n",
      "treatment_tags:[]\n",
      "ups:21\n",
      "upvote_ratio:0.8500000238418579\n",
      "url:https://www.reddit.com/r/nba/comments/1hqr92b/game_thread_minnesota_timberwolves_1714_oklahoma/\n",
      "user_reports:[]\n",
      "view_count:None\n",
      "visited:False\n",
      "wls:6\n"
     ]
    }
   ],
   "source": [
    "print(f'In total {len(data[0].keys())} keys')\n",
    "for i in data[0].keys():\n",
    "    print(f'{i}:{data[0][i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47a48bb3-8c04-4538-ae7b-187622fb262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(data, columns):\n",
    "    return_data = []\n",
    "    for i in data:\n",
    "        filtered_data = {}\n",
    "        for k, v in i.items():\n",
    "            if type(v) == list:\n",
    "                if len(v) == 0:\n",
    "                    v = None\n",
    "                else:\n",
    "                    v = str(v)\n",
    "            if type(v) == dict:\n",
    "                v = str(v)\n",
    "            \n",
    "            if k in columns:\n",
    "                filtered_data[k] = v\n",
    "        return_data.append(filtered_data)\n",
    "\n",
    "    df_return = pd.DataFrame(return_data)\n",
    "    return_data = []\n",
    "    return df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e86b97-36b8-4b68-9fe5-29d6c2780337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Reddit post data categories\n",
    "metadata_keys = [\"id\", \"name\", \"permalink\", \"url\", \"created_utc\", \"retrieved_on\", \"_meta.retrieved_2nd_on\"]\n",
    "post_content_keys = [\"id\", \"title\", \"selftext\", \"domain\", \"is_self\", \"is_video\", \"media\", \"secure_media\"]\n",
    "engagement_keys = [\"num_comments\", \"score\", \"ups\", \"downs\", \"upvote_ratio\", \"view_count\"]\n",
    "author_keys = [\"author\", \"author_fullname\", \"author_premium\", \"author_flair_text\", \"author_flair_css_class\", \"author_flair_richtext\", \"author_flair_background_color\", \"author_flair_text_color\", \"author_flair_type\"]\n",
    "subreddit_keys = [\"subreddit\", \"subreddit_id\", \"subreddit_name_prefixed\", \"subreddit_type\", \"subreddit_subscribers\"]\n",
    "moderation_keys = [\"archived\", \"locked\", \"stickied\", \"removed_by\", \"removed_by_category\", \"banned_by\", \"banned_at_utc\", \"mod_reports\", \"num_reports\", \"report_reasons\", \"removal_reason\", \"quarantine\", \"approved_by\", \"approved_at_utc\"]\n",
    "awards_flair_keys = [\"all_awardings\", \"awarders\", \"gilded\", \"total_awards_received\", \"top_awarded_type\", \"gildings\", \"link_flair_text\", \"link_flair_richtext\", \"link_flair_type\", \"link_flair_background_color\", \"link_flair_text_color\", \"link_flair_css_class\"]\n",
    "visibility_keys = [\"over_18\", \"spoiler\", \"pinned\", \"contest_mode\", \"hide_score\", \"hidden\", \"no_follow\", \"can_gild\", \"can_mod_post\", \"is_crosspostable\", \"is_robot_indexable\", \"is_original_content\", \"is_reddit_media_domain\", \"is_created_from_ads_ui\", \"discussion_type\", \"suggested_sort\", \"visited\", \"saved\", \"send_replies\", \"pwls\", \"wls\", \"media_only\", \"content_categories\", \"treatment_tags\"]\n",
    "\n",
    "cols = [metadata_keys, post_content_keys, engagement_keys, author_keys, subreddit_keys, moderation_keys, moderation_keys, awards_flair_keys, visibility_keys]\n",
    "cols_names = ['metadata_keys', 'post_content_keys', 'engagement_keys', 'author_keys', 'subreddit_keys', 'moderation_keys', 'moderation_keys',  'awards_flair_keys', 'visibility_keys']\n",
    "conn = sqlite3.connect('reddit_posts.db')\n",
    "for i in tqdm(range(len(cols)), desc='Making SQL database'):\n",
    "    col = cols[i]\n",
    "    col_name = cols_names[i]\n",
    "    col.insert(0, 'id')\n",
    "    col = list(set(col))\n",
    "    df = make_df(data, col) \n",
    "    df.to_sql(col_name, conn, if_exists='replace', index=False)\n",
    "print('SQL database successfully made. Written to reddit_posts.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d45620-9a71-40a1-a653-f47bfe2ab35b",
   "metadata": {},
   "source": [
    "## Find nested JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "778549f1-e334-48ae-bfbc-6caba27f109a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading lines...: 19it [00:00, 8561.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🔍 Non-empty Nested JSON found in keys: ['gallery_data', 'gallery_data.items', 'gallery_data.items', 'gallery_data.items', 'media_metadata', 'media_metadata.066pukz7w9ae1', 'media_metadata.066pukz7w9ae1.p', 'media_metadata.066pukz7w9ae1.p', 'media_metadata.066pukz7w9ae1.p', 'media_metadata.066pukz7w9ae1.p', 'media_metadata.066pukz7w9ae1.p', 'media_metadata.066pukz7w9ae1.p', 'media_metadata.066pukz7w9ae1.s', 'media_metadata.ht8b3l08w9ae1', 'media_metadata.ht8b3l08w9ae1.p', 'media_metadata.ht8b3l08w9ae1.p', 'media_metadata.ht8b3l08w9ae1.p', 'media_metadata.ht8b3l08w9ae1.p', 'media_metadata.ht8b3l08w9ae1.p', 'media_metadata.ht8b3l08w9ae1.p', 'media_metadata.ht8b3l08w9ae1.s', 'media_metadata.ira4rxz7w9ae1', 'media_metadata.ira4rxz7w9ae1.p', 'media_metadata.ira4rxz7w9ae1.p', 'media_metadata.ira4rxz7w9ae1.p', 'media_metadata.ira4rxz7w9ae1.p', 'media_metadata.ira4rxz7w9ae1.p', 'media_metadata.ira4rxz7w9ae1.p', 'media_metadata.ira4rxz7w9ae1.s']\n",
      "\n",
      "{\n",
      "    \"_meta\": {\n",
      "        \"retrieved_2nd_on\": 1735819210\n",
      "    },\n",
      "    \"all_awardings\": [],\n",
      "    \"allow_live_comments\": false,\n",
      "    \"approved_at_utc\": null,\n",
      "    \"approved_by\": null,\n",
      "    \"archived\": false,\n",
      "    \"author\": \"GTPowers\",\n",
      "    \"author_flair_background_color\": null,\n",
      "    \"author_flair_css_class\": null,\n",
      "    \"author_flair_richtext\": [],\n",
      "    \"author_flair_template_id\": null,\n",
      "    \"author_flair_text\": null,\n",
      "    \"author_flair_text_color\": null,\n",
      "    \"author_flair_type\": \"text\",\n",
      "    \"author_fullname\": \"t2_z1q6p\",\n",
      "    \"author_is_blocked\": false,\n",
      "    \"author_patreon_flair\": false,\n",
      "    \"author_premium\": false,\n",
      "    \"awarders\": [],\n",
      "    \"banned_at_utc\": null,\n",
      "    \"banned_by\": null,\n",
      "    \"can_gild\": false,\n",
      "    \"can_mod_post\": false,\n",
      "    \"category\": null,\n",
      "    \"clicked\": false,\n",
      "    \"content_categories\": null,\n",
      "    \"contest_mode\": false,\n",
      "    \"created\": 1735689601,\n",
      "    \"created_utc\": 1735689601,\n",
      "    \"discussion_type\": null,\n",
      "    \"distinguished\": null,\n",
      "    \"domain\": \"reddit.com\",\n",
      "    \"downs\": 0,\n",
      "    \"edited\": false,\n",
      "    \"gallery_data\": {\n",
      "        \"items\": [\n",
      "            {\n",
      "                \"id\": 577730306,\n",
      "                \"media_id\": \"066pukz7w9ae1\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 577730307,\n",
      "                \"media_id\": \"ira4rxz7w9ae1\"\n",
      "            },\n",
      "            {\n",
      "                \"id\": 577730308,\n",
      "                \"media_id\": \"ht8b3l08w9ae1\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"gilded\": 0,\n",
      "    \"gildings\": {},\n",
      "    \"hidden\": false,\n",
      "    \"hide_score\": false,\n",
      "    \"id\": \"1hqr931\",\n",
      "    \"is_created_from_ads_ui\": false,\n",
      "    \"is_crosspostable\": true,\n",
      "    \"is_gallery\": true,\n",
      "    \"is_meta\": false,\n",
      "    \"is_original_content\": false,\n",
      "    \"is_reddit_media_domain\": false,\n",
      "    \"is_robot_indexable\": true,\n",
      "    \"is_self\": false,\n",
      "    \"is_video\": false,\n",
      "    \"likes\": null,\n",
      "    \"link_flair_background_color\": \"#99e7ff\",\n",
      "    \"link_flair_css_class\": \"post-type-1\",\n",
      "    \"link_flair_richtext\": [\n",
      "        {\n",
      "            \"e\": \"text\",\n",
      "            \"t\": \"Type 1\"\n",
      "        }\n",
      "    ],\n",
      "    \"link_flair_template_id\": \"cfa8d4d4-df3d-11e2-b662-12313d05241f\",\n",
      "    \"link_flair_text\": \"Type 1\",\n",
      "    \"link_flair_text_color\": \"dark\",\n",
      "    \"link_flair_type\": \"richtext\",\n",
      "    \"locked\": false,\n",
      "    \"media\": null,\n",
      "    \"media_embed\": {},\n",
      "    \"media_metadata\": {\n",
      "        \"066pukz7w9ae1\": {\n",
      "            \"e\": \"Image\",\n",
      "            \"id\": \"066pukz7w9ae1\",\n",
      "            \"m\": \"image/jpg\",\n",
      "            \"p\": [\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/066pukz7w9ae1.jpg?width=108&crop=smart&auto=webp&s=a1801396d98128bc3475a392e4331303c4696307\",\n",
      "                    \"x\": 108,\n",
      "                    \"y\": 72\n",
      "                },\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/066pukz7w9ae1.jpg?width=216&crop=smart&auto=webp&s=1bb92f940fec0f4917dcd3bbc3dba5cb65cd6eb2\",\n",
      "                    \"x\": 216,\n",
      "                    \"y\": 144\n",
      "                },\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/066pukz7w9ae1.jpg?width=320&crop=smart&auto=webp&s=0741e8baefada5fe4b68177519048079c34adba3\",\n",
      "                    \"x\": 320,\n",
      "                    \"y\": 213\n",
      "                },\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/066pukz7w9ae1.jpg?width=640&crop=smart&auto=webp&s=70aa6aeb7c52f69db7de9cfe3b8842eac6732531\",\n",
      "                    \"x\": 640,\n",
      "                    \"y\": 426\n",
      "                },\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/066pukz7w9ae1.jpg?width=960&crop=smart&auto=webp&s=d6fd9b671d638580b061eb07259cf7dd41ca1857\",\n",
      "                    \"x\": 960,\n",
      "                    \"y\": 640\n",
      "                },\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/066pukz7w9ae1.jpg?width=1080&crop=smart&auto=webp&s=e33a7b696b8757da4bb763f770fb8cd1631580da\",\n",
      "                    \"x\": 1080,\n",
      "                    \"y\": 720\n",
      "                }\n",
      "            ],\n",
      "            \"s\": {\n",
      "                \"u\": \"https://preview.redd.it/066pukz7w9ae1.jpg?width=4740&format=pjpg&auto=webp&s=f7fed40d4626d67a397155dd70b25dc8846bc12e\",\n",
      "                \"x\": 4740,\n",
      "                \"y\": 3160\n",
      "            },\n",
      "            \"status\": \"valid\"\n",
      "        },\n",
      "        \"ht8b3l08w9ae1\": {\n",
      "            \"e\": \"Image\",\n",
      "            \"id\": \"ht8b3l08w9ae1\",\n",
      "            \"m\": \"image/jpg\",\n",
      "            \"p\": [\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/ht8b3l08w9ae1.jpg?width=108&crop=smart&auto=webp&s=59e8a56e5060072e5904362d3805b6e59031446e\",\n",
      "                    \"x\": 108,\n",
      "                    \"y\": 161\n",
      "                },\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/ht8b3l08w9ae1.jpg?width=216&crop=smart&auto=webp&s=926fc48e5c093d4e014814f7791e18407ab338c9\",\n",
      "                    \"x\": 216,\n",
      "                    \"y\": 323\n",
      "                },\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/ht8b3l08w9ae1.jpg?width=320&crop=smart&auto=webp&s=c44406b930d5e4af0abf89abe07cafded2dcd081\",\n",
      "                    \"x\": 320,\n",
      "                    \"y\": 479\n",
      "                },\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/ht8b3l08w9ae1.jpg?width=640&crop=smart&auto=webp&s=6cb89bfc743f72b816a05c0e5f8f0792428061a2\",\n",
      "                    \"x\": 640,\n",
      "                    \"y\": 959\n",
      "                },\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/ht8b3l08w9ae1.jpg?width=960&crop=smart&auto=webp&s=9e9cf369105779edc80e73741ee4559c74f5a8aa\",\n",
      "                    \"x\": 960,\n",
      "                    \"y\": 1439\n",
      "                },\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/ht8b3l08w9ae1.jpg?width=1080&crop=smart&auto=webp&s=b6cfc7b46592fc1a60664ce6ef39d80b75192469\",\n",
      "                    \"x\": 1080,\n",
      "                    \"y\": 1619\n",
      "                }\n",
      "            ],\n",
      "            \"s\": {\n",
      "                \"u\": \"https://preview.redd.it/ht8b3l08w9ae1.jpg?width=2993&format=pjpg&auto=webp&s=b87b18ebf0abe203e34559891dc3207f6e6dcd70\",\n",
      "                \"x\": 2993,\n",
      "                \"y\": 4489\n",
      "            },\n",
      "            \"status\": \"valid\"\n",
      "        },\n",
      "        \"ira4rxz7w9ae1\": {\n",
      "            \"e\": \"Image\",\n",
      "            \"id\": \"ira4rxz7w9ae1\",\n",
      "            \"m\": \"image/jpg\",\n",
      "            \"p\": [\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/ira4rxz7w9ae1.jpg?width=108&crop=smart&auto=webp&s=86d827277fc809f7e634ca627c3dd54f447a49d6\",\n",
      "                    \"x\": 108,\n",
      "                    \"y\": 72\n",
      "                },\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/ira4rxz7w9ae1.jpg?width=216&crop=smart&auto=webp&s=bb8bb858bb98ac3c9e9e7f385b57ab47c7625b5e\",\n",
      "                    \"x\": 216,\n",
      "                    \"y\": 144\n",
      "                },\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/ira4rxz7w9ae1.jpg?width=320&crop=smart&auto=webp&s=52d077ba8a1a7c71c56a35e9a34b67109062114c\",\n",
      "                    \"x\": 320,\n",
      "                    \"y\": 213\n",
      "                },\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/ira4rxz7w9ae1.jpg?width=640&crop=smart&auto=webp&s=ec68d26de8f394f98cfac20831b0c2f86b79028f\",\n",
      "                    \"x\": 640,\n",
      "                    \"y\": 426\n",
      "                },\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/ira4rxz7w9ae1.jpg?width=960&crop=smart&auto=webp&s=554b669460417811163e3a9cff993a8dcc395575\",\n",
      "                    \"x\": 960,\n",
      "                    \"y\": 640\n",
      "                },\n",
      "                {\n",
      "                    \"u\": \"https://preview.redd.it/ira4rxz7w9ae1.jpg?width=1080&crop=smart&auto=webp&s=c14ed893764e001a860d0f6453251e705d62912c\",\n",
      "                    \"x\": 1080,\n",
      "                    \"y\": 720\n",
      "                }\n",
      "            ],\n",
      "            \"s\": {\n",
      "                \"u\": \"https://preview.redd.it/ira4rxz7w9ae1.jpg?width=4678&format=pjpg&auto=webp&s=b7880e22d2ceaf3ee5a82ab195709510c06e39bc\",\n",
      "                \"x\": 4678,\n",
      "                \"y\": 3119\n",
      "            },\n",
      "            \"status\": \"valid\"\n",
      "        }\n",
      "    },\n",
      "    \"media_only\": false,\n",
      "    \"mod_note\": null,\n",
      "    \"mod_reason_by\": null,\n",
      "    \"mod_reason_title\": null,\n",
      "    \"mod_reports\": [],\n",
      "    \"name\": \"t3_1hqr931\",\n",
      "    \"no_follow\": false,\n",
      "    \"num_comments\": 25,\n",
      "    \"num_crossposts\": 0,\n",
      "    \"num_reports\": null,\n",
      "    \"over_18\": false,\n",
      "    \"permalink\": \"/r/diabetes/comments/1hqr931/t1d_dadbag/\",\n",
      "    \"pinned\": false,\n",
      "    \"pwls\": null,\n",
      "    \"quarantine\": false,\n",
      "    \"removal_reason\": null,\n",
      "    \"removed_by\": null,\n",
      "    \"removed_by_category\": null,\n",
      "    \"report_reasons\": null,\n",
      "    \"retrieved_on\": 1735689620,\n",
      "    \"saved\": false,\n",
      "    \"score\": 58,\n",
      "    \"secure_media\": null,\n",
      "    \"secure_media_embed\": {},\n",
      "    \"selftext\": \"\",\n",
      "    \"send_replies\": true,\n",
      "    \"spoiler\": false,\n",
      "    \"stickied\": false,\n",
      "    \"subreddit\": \"diabetes\",\n",
      "    \"subreddit_id\": \"t5_2qhsj\",\n",
      "    \"subreddit_name_prefixed\": \"r/diabetes\",\n",
      "    \"subreddit_subscribers\": 134962,\n",
      "    \"subreddit_type\": \"public\",\n",
      "    \"suggested_sort\": \"confidence\",\n",
      "    \"thumbnail\": \"https://b.thumbs.redditmedia.com/gzKyHSR3EaVHzKoCKRQBzy6tICUwM_B3FtSMi9jVwMM.jpg\",\n",
      "    \"thumbnail_height\": 93,\n",
      "    \"thumbnail_width\": 140,\n",
      "    \"title\": \"T1D Dad-bag\",\n",
      "    \"top_awarded_type\": null,\n",
      "    \"total_awards_received\": 0,\n",
      "    \"treatment_tags\": [],\n",
      "    \"ups\": 58,\n",
      "    \"upvote_ratio\": 0.9599999785423279,\n",
      "    \"url\": \"https://www.reddit.com/gallery/1hqr931\",\n",
      "    \"url_overridden_by_dest\": \"https://www.reddit.com/gallery/1hqr931\",\n",
      "    \"user_reports\": [],\n",
      "    \"view_count\": null,\n",
      "    \"visited\": false,\n",
      "    \"wls\": null\n",
      "}\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "submission_file = 'reddit/submissions/RS_2025-01/RS_2025-01'\n",
    "\n",
    "def find_non_empty_nested_keys(obj, parent_key=\"\"):\n",
    "    \"\"\"Recursively find non-empty nested JSON objects (length > 0), excluding items in list excluding.\"\"\"\n",
    "    nested_keys = []\n",
    "\n",
    "    excluding = ['_meta', 'author_flair_richtext', 'link_flair_richtext', 'preview', 'crosspost_parent_list']\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            if key in excluding:  # Exclude keys in the excluding list\n",
    "                continue\n",
    "            if isinstance(value, dict) and len(value) > 0:  # Non-empty dictionary\n",
    "                nested_keys.append(parent_key + key if parent_key else key)\n",
    "                nested_keys.extend(find_non_empty_nested_keys(value, parent_key + key + \".\"))  # Recurse\n",
    "            elif isinstance(value, list):  # List containing non-empty dicts\n",
    "                for i, item in enumerate(value):\n",
    "                    if isinstance(item, dict) and len(item) > 0:\n",
    "                        nested_keys.append(parent_key + key if parent_key else key)\n",
    "                        nested_keys.extend(find_non_empty_nested_keys(item, parent_key + key + f\"[{i}].\"))  # Recurse\n",
    "\n",
    "    elif isinstance(obj, list):\n",
    "        for i, item in enumerate(obj):\n",
    "            if isinstance(item, dict) and len(item) > 0:\n",
    "                nested_keys.extend(find_non_empty_nested_keys(item, parent_key + f\"[{i}].\"))  # Recurse\n",
    "\n",
    "    return nested_keys\n",
    "\n",
    "with open(submission_file, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f, desc='Reading lines...'):\n",
    "        line = json.loads(line)  # Parse JSON\n",
    "        nested_keys = find_non_empty_nested_keys(line)  # Find non-empty nested JSON\n",
    "        \n",
    "        if nested_keys:  # Only print if nested objects have length > 0\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(f\"🔍 Non-empty Nested JSON found in keys: {nested_keys}\\n\")\n",
    "            print(json.dumps(line, indent=4, ensure_ascii=False))  # Pretty-print JSON\n",
    "            print(\"=\" * 80 + \"\\n\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
